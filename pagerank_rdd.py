from pyspark.sql import SparkSession
from operator import add
import sys

DAMPING = 0.85
ITERATIONS = 10


def main(input_path, sample_ratio=1.0):
    spark = SparkSession.builder \
        .appName("PageRank-RDD-CSV") \
        .getOrCreate()

    sc = spark.sparkContext
    sc.setLogLevel("WARN")

    print(f"\nüîµ Lecture du fichier : {input_path}")
    print(f"üîµ Sample ratio (10% = 0.1) : {sample_ratio}\n")

    # 1Ô∏è‚É£ Lire edges.csv (src,dst)
    # Format attendu : <url1>,<url2>
    edges = (
        spark.read.csv(input_path, header=False)
             .rdd
             .map(lambda row: (row[0], row[1]))       # (src, dst)
             .filter(lambda x: x[0] is not None and x[1] is not None)
    )

    # 2Ô∏è‚É£ Appliquer un √©chantillonnage (pour 10% dans le cloud)
    if 0 < sample_ratio < 1:
        edges = edges.sample(False, sample_ratio, seed=42)

    edges = edges.distinct().cache()

    print(f"üîµ Nombre total d'edges : {edges.count()}")

    # 3Ô∏è‚É£ Construire la liste d'adjacence
    links = edges.groupByKey().mapValues(list).cache()

    # Partitionnement pour √©viter les shuffles sur links
    num_partitions = links.getNumPartitions()
    links = links.partitionBy(num_partitions).cache()

    # 4Ô∏è‚É£ Pages uniques
    pages = edges.flatMap(lambda e: [e[0], e[1]]).distinct()
    print(f"üîµ Nombre de pages uniques : {pages.count()}\n")

    # Initialisation des ranks = 1.0
    ranks = (
        pages.map(lambda p: (p, 1.0))
             .partitionBy(num_partitions)
             .cache()
    )

    print("\n===== üöÄ D√©but du calcul PageRank (RDD) =====\n")

    # 5Ô∏è‚É£ Boucle PageRank
    for i in range(ITERATIONS):
        print(f"üîÅ It√©ration {i+1}/{ITERATIONS}")

        # join(links, ranks) ne provoque PAS de shuffle car partitionnement identique
        contribs = (
            links.join(ranks)
                 .flatMap(
                    lambda pr: [
                        (dst, pr[1][1] / len(pr[1][0]))
                        for dst in pr[1][0]
                    ]
                 )
        )

        # Somme des contributions (ce shuffle est in√©vitable)
        ranks = (
            contribs.reduceByKey(add)
                    .mapValues(lambda s: (1 - DAMPING) + DAMPING * s)
                    .partitionBy(num_partitions)
                    .cache()
        )

        # Affichage d√©but + fin
        if i < 2 or i == ITERATIONS - 1:
            print("Top pages √† cette it√©ration :")
            for p, r in ranks.takeOrdered(5, key=lambda x: -x[1]):
                print(f"  {p:<40} {r}")
            print()

    # 6Ô∏è‚É£ R√©sultat final
    print("\n===== üî• TOP 20 R√âSULTATS FINAUX üî• =====")
    top20 = ranks.takeOrdered(20, key=lambda x: -x[1])
    for p, r in top20:
        print(f"{p:<50} {r}")

    print("\nüéâ FIN ‚Äî PageRank RDD ex√©cut√© correctement !\n")
    spark.stop()


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: spark-submit pagerank_rdd.py <edges.csv> [sample_ratio]")
        sys.exit(1)

    input_path = sys.argv[1]
    sample_ratio = float(sys.argv[2]) if len(sys.argv) > 2 else 1.0
    main(input_path, sample_ratio)
